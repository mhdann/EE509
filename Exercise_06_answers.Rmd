---
title: "Lab 06 - MCMC"
author: "Michael Dann"
date: "October 20, 2014"
output: html_document
---

The goal of this week's lab is to get under the hood?? of Bayesian numerical methods in order to understand what is going on behind the scenes in BUGS, where output just magically appears.  It is also important to be able to fall back on doing these computations explicitly because we will occasionally find ourselves faced with models that are too complex or too large for BUGS.  For this week's lab we will return to using R.

## Bayesian Regression using Gibbs Sampling

Linear regression is a good place to start our exploration of numerical methods because it is the foundation for the large majority of the data analysis that occurs using classical methods (recall that ANOVA models are just a special case of regression) and because it gives us a foundation to build off of for exploring more complex models.

Recall from lecture 13 and chapter 7.4 of the textbook that the standard Bayesian regression model assumes a Normal likelihood, a Normal prior on the regression parameters, and an Inverse Gamma prior on the variance.

$$P(b,\sigma^2 \vert X, y) \propto N_n(y \vert Xb,\sigma^2 I) N_p(b \vert b_0, V_b) IG(\sigma^2 \vert s_1,s_2)$$

Within the Gibbs sampler we will be iteratively sampling from each of the conditional posterior distributions:

  The regression parameters given the variance

$$P(b \vert \sigma^2, X, y) \propto N_n(y \vert Xb,\sigma^2 I) N_p(b \vert b_0, V_b)$$

  The variance given the regression parameters

$$P(\sigma^2 \vert b, X, y) \propto N_p(b \vert b_0, V_b) IG(\sigma^2 \vert s_1,s_2)$$

We can divide the R code required to perform this analysis into three parts:

* Code used to set-up the analysis
    + load data
    + specify parameters for the priors
    + set up variables to store MCMC
    + specify initial conditions
* MCMC loop
* Code used to evaluate the analysis
    + Convergence diagnostics
    + Summary statistics
    + Credible & predictive intervals

As a reminder, we **strongly** recommend that you assemble the bits of code from the lab into a single Rmd script file.  This will make re-running parts of the code much easier since you will not have to cut-and-paste and it provides you with a script to start from if you want to use the code in the future for your own data analysis.  Also recall from Lab 1 that the functions save(), save.image(), and load() allow you save the output of your analysis and reload it later.

## Set up

For the first part of this lab we are going to be simulating data from a known model instead of using a real data set.  This exercise serves two purposes;? first, to allow us to evaluate the MCMC's ability to recover the true?? parameters of the model and second to demonstrate how pseudo-data?? can be generated.  Pseudo-data experiments?? can be useful for assessing the power of a test/statistical model and for exploring experimental design issues (e.g. how many samples do I need, how should I distribute them?).  In a real analysis you would substitute code that loads up your data from a file instead of the following bit of code

```{r}
### Part 1: simulate data from a known model

# Assign the parameters
b0 <- 10				## define the intercept
b1 <- 2					## define the slope
beta <- matrix(c(b0,b1),2,1)		## put true?? regression parameters in a matrix
sigma2 <- 4^2				## define the variance (s.d. = 4)
n <- 500        ## define the sample size

gen_data <- function(n, beta, sigma2){
  # Generate sample data
  x1 <- runif(n,0,20)
  x <- cbind(rep(1,n),x1)
  y <- matrix(rnorm(n, x%*%beta, sqrt(sigma2)),n,1)
  return(list(x1=x1,y=y, x= x))
}
data <- gen_data(n, beta, sigma2)

# visual check
plot(data$x1,data$y)
abline(b0,b1,col=2,lwd=3)
```


Now that we have data??, the next task in setting up the analysis is to specify the parameters for the prior distributions.  Here we'll assume that beta has a Normal prior with mean bprior and a variance Vb.  Since beta is a vector, the prior mean, bprior, is also a vector, and the prior variance is a matrix.  Since we have no prior conceptions about our data we'll select relatively weak priors, assuming a prior mean of 0 and a prior variance matrix that is diagonal (i.e. no covariances) and with a moderately large variance of 1000 (s.d. of 31.6).  We'll use the _diag_ command to set up a diagonal matrix that has a size of 2 and values of 1000 along the diagonal.  In practice we never actually need to use the prior variance matrix, but we will frequently use the inverse of the matrix.  Therefore we will compute the inverse of the prior variance matrix, vinvert,  using the _solve_ function.  Finally we will specify an uninformative inverse gamma prior on the variance with parameters s1 = 0.1 and s2 = 0.1.

```{r}
## specify priors
append_priors <- function(data, bprior, vinvert,s1,s2){
  data$bprior <-bprior
  data$vinvert <-vinvert
  data$s1 <- s1
  data$s2 <- s2
  return(data)
}
bprior <- as.vector(c(0,0))
vinvert <- solve(diag(1000,2))
s1 <- 0.1
s2 <- 0.1
data <- append_priors(data, bprior, vinvert, s1, s2)

##precompute frequently used quantities
precompute <-function(data){
  data$VbB = data$vinvert %*% data$bprior
  data$XY = t(data$x) %*% data$y
  data$XX = t(data$x) %*% data$x
  return(data)
}
data <- precompute(data)

##load libraries
library(coda)
library(mvtnorm)

## initial conditions
sg <- 50
sinv <- 1/sg
```


## MCMC loop

The core of this analysis is the main loop of the MCMC where we will iteratively draw from the posterior distribution for the regression parameters conditioned on the variance and the variance conditioned on the regression parameters.  All of this occurs within a large loop that counts the number of iterations and stores the current value of the parameter values.  We'll begin with  the R code for the overall structure of the MCMC and then fill in the details for each sampler

```
## Gibbs loop
for(g in 1:ngibbs){

  ## sample regression parameters
  ## See second equation above in the first section
   
  
  ## sample variance
  ## See third equation above in the first section
<<insert inverse gamma here >>

  ## storage
  bgibbs[g,] <- b  ## store the current value of beta vector
  sgibbs[g]  <- sg	## store the current value of the variance

  if(g %%100 == 0) print(g)	##show how many steps have been performed
}
```


Recall from lecture 13 and from Section 7.4 in the textbook that conditional posterior for the regression parameters 

$$P(b \vert \sigma^2, X, y) \propto N_n(y \vert Xb,\sigma^2 I) N_p(b \vert b_0, V_b)$$

has a multivariate normal posterior that takes on the form

$$p(b \vert \sigma^2, X, y) \propto N_p(b \vert Vv , V)$$

where

$$V^{-1} = \sigma^{-2} X^T X + V_b^{-1}$$
$$v = \sigma^{-2} X^t y + V_b^{-1} b_0$$

We can implement this sampler in R as

```
 ## sample regression parameters
  bigV    <- solve(sinv*XX + vinvert)  ## Covariance matrix
  littlev <- sinv*XY + VbB
  b = t(rmvnorm(1,bigV %*% littlev,bigV))   ## Vv is the mean vector
```

where **rmvnorm** is the multivariate version of rnorm that takes a mean vector and a covariance matrix as it's 2nd and 3rd arguments.  We'd recommend taking a look at the output of each step of these calculations to get a feel for what is being calculated.  This code should be cut-and-pasted into the overall MCMC loop above.

Next lets look at the sampler for the variance term, which has a posterior distribution

$$P(\sigma^2 \vert b, X, y) \propto N_p(b \vert b_0, V_b) IG(\sigma^2 \vert s_1,s_2)$$

that takes on an Inverse Gamma posterior

$$IG(\sigma^2 \vert u_1,u_2) \propto \left( \sigma^2 \right)^{-(u_1+1)}exp \left[ -{u_2}\over{\sigma^2}  \right]$$

where $u_1 = s_1 + n/2$ and $u_2 = s_2 + {{1}\over{2}}(y-Xb)^T(y-Xb)$

We can implement this in R as

```
  ## sample variance
  u1 <- s1 + n/2
  u2 <- s2 + 0.5*crossprod(y-x%*%b)
  sinv <- rgamma(1,u1,u2)
  sg <- 1/sinv
```


Since R does not have a built in inverse gamma distribution we instead sample the inverse of the variance (aka the precision) as a gamma, and then compute the variance in the final step.  Also note the introduction of the crossprod(A) function that calculates $A^T A$ and is slightly more efficient than doing the calculation explictly as `t(A) %*% A`.  This bit of code should also be cut-and-pasted into the overall MCMC loop.

At this point you should run the MCMC loop to check for errors and take a look at the output (bgibbs and sgibbs) to see if values of the parameters make sense.  Once you are confident in the results, you should increase ngibbs to a larger values (e.g. ngibbs <- 10000) and re-run the script from that point through the MCMC loop.

```{r}
## Gibbs loop
gibbs <- function(data, sinv){
  data$ngibbs <- 10000  ## number of updates
  data$bgibbs <- matrix(0.0,nrow=data$ngibbs,ncol=2) 	## storage for beta
  data$sgibbs <- numeric(data$ngibbs)			## storage for sigma2
  for(g in 1:data$ngibbs){
  
    ## sample regression parameters
    ## See second equation above in the first section
    bigV    <- solve(sinv*data$XX + data$vinvert)  ## Covariance matrix
    littlev <- sinv*data$XY + data$VbB
    b = t(rmvnorm(1,bigV %*% littlev,bigV))   ## Vv is the mean vector
    
    ## sample variance
    ## See third equation above in the first section
    u1 <- data$s1 + n/2
    u2 <- data$s2 + 0.5*crossprod(data$y-data$x%*%b)
    sinv <- rgamma(1,u1,u2)
    sg <- 1/sinv
  
    ## storage
    data$bgibbs[g,] <- b  ## store the current value of beta vector
    data$sgibbs[g]  <- sg  ## store the current value of the variance  
    #if(g %%100 == 0) print(g)	##show how many steps have been performed
  }  
  return(data)
}

output <- gibbs(data, sinv)
```


## Evaluation

As we did in BUGS, we need to evaluate the output of the MCMC.  We will make use of the *coda*?? library to simplify these graphs and calculations. Let'??s first look at the output for the regression parameters.

### Lab Report Task 1
* Evaluate the MCMC chain for the variance.  Include relevant diagnostic plots.
* Compare the summary statistics for the Bayesian regression model to those from the classical regression:  summary(lm( y ~ x1 )).  This should include a comparison of the means and uncertainties of **all 3 model parameters**
* Compare the fit parameters to the *true*?? parameters we used to generate the pseudo-data.  How well does the statistical analysis recover the true model?
```{r}

## diagnostics of beta
bmcmc <- mcmc(output$bgibbs)  ## use beta
vmcmc <- mcmc(output$sgibbs)   # use the variance

plot(bmcmc)  		## mcmc history and density plot
title("Beta",outer=T)
autocorr.plot(bmcmc)		## autocorrelation
title("Beta",outer=T)
cumuplot(bmcmc)		## quantile plot
title("Beta",outer=T)
plot(output$bgibbs[,1],output$bgibbs[,2], main="Beta1 v Beta0", xlab="Beta0", ylab="Beta1")  ## pairs plot to evaluate parameter correlation


# Variance plots
plot(vmcmc)  		        ## mcmc history and density plot
title("Variance",outer=T)
autocorr.plot(vmcmc)		## autocorrelation
title("Variance",outer=T)
cumuplot(vmcmc)		      ## quantile plot
title("Variance",outer=T)

summary(bmcmc)           ## summary table coefs
summary(vmcmc)         	## summary table var
summary(lm( data$y ~ data$x1 ))
print(c(b0,b1,sigma2))
```



## Regression Credible Intervals

When fitting the mean of a distribution, the density plot of that distribution and its moments/quantiles provides all the information we need to evaluate the performance of the model.  By contrast, when fitting a process model that has covariates, whether it be a simple regression or a complex nonlinear model, we are also often interested in the error estimate on the overall model.  This error estimate comes in two forms, the credible interval and the prediction interval.  The credible interval, which is the Bayesian analog to the frequentist confidence interval, provides an uncertainty estimate based on the uncertainty in the model parameters.  We have already seen Bayesian credible intervals frequently as the quantiles of the posterior distribution of individual parameters.  We can extend this to an estimate of model uncertainty by looking at the uncertainty in the distribution of the model itself by calculating a credible interval around the regression line.  Numerically we do this by evaluating the model over a sequence of covariate values for every pair of parameter values in the MCMC sequence.  Lets begin by looking at a subset of the MCMC 

```{r}
output$xpred <- 0:20
plot(output$x1,output$y)
for(i in 1:10){
  lines(output$xpred, output$bgibbs[i,1] + output$bgibbs[i,2]*output$xpred)
}
```


You can see within the loop that we're plotting our process model $b0 + b1*x$? for pairs of regression parameters from the MCMC which created a distribution of models.  The reason that we use pairs of values from the posterior (i.e. rows in bgibbs) rather than simulating values from the posterior of each parameter independently is to account for the covariance structure of the parameters.  As we saw in the pairs plot above, the covariance of the slope and intercept is considerable, and thus independent sapling of their marginal posterior distributions would lead to credible intervals that are substantially too large.
	This distribution of models is by itself not easy to interpret, especially if we added lines for EVERY pair of points in our posterior, so instead we'll calculate the quantiles of the posterior distribution of all of these lines.  To do this we'll need to first make the predictions for all of these lines and then look at the posterior distribution of predicted y values given our sequence of x values.
	Before we dive into that, lets also consider the other quantity we're interested in calculating, the predictive interval, since it is easiest to calculate both at the same time.  While the credible interval was solely concerned about the uncertainty in the model parameters, the predictive interval is also concerned about the residual error between the model and the data.  When we make a prediction with a model and want to evaluate how well the model matches data we obviously need to consider our data model.  How we do this numerically is to generate pseudodata from our model, conditioned on the current value of not only the regression parameters but also the variance parameters, and then look at the distribution of predictions.
	In R we'll begin the calculation of our credible and predictive intervals by setting up data structures to store all the calculations

```{r}
  
credi_predi <- function(data){
  ## credible and prediction intervals for the mcmc object
  data$xpred <- 0:20    				      ## sequence of x values we're going to
  data$npred <- length(data$xpred)				##      make predictions for
  data$ypred <- matrix(0.0,nrow=data$ngibbs,ncol=data$npred)	## storage for predictive interval
  data$ycred <- matrix(0.0,nrow=data$ngibbs,ncol=data$npred)	## storage for credible interval
  
  beg = 1
  thin = 1
  
  for(g in seq(from=beg,to=data$ngibbs,by=thin)){
      data$Ey <- data$bgibbs[g,1] + data$bgibbs[g,2] * data$xpred
      data$ycred[g,] <- data$Ey
      data$ypred[g,] <- rnorm(data$npred,data$Ey,sqrt(data$sgibbs[g]))
  }
  
  data$ci <- apply(data$ycred,2,quantile,c(0.025,0.5,0.975))  ## credible interval and median
  data$pi <- apply(data$ypred,2,quantile,c(0.025,0.975))		## prediction interval
  return(data)
}

output <- credi_predi(output)

plot_stuff <- function(data){  
  plot(data$x1,data$y,cex=0.5,xlim=c(0,20),ylim=c(0,50),xlab="x",ylab="y", main=paste("Sample size", length(data$x1)))
  lines(data$xpred,data$ci[1,],col=3,lty=2)  ## lower CI
  lines(data$xpred,data$ci[2,],col=3,lwd=2)	## median
  lines(data$xpred,data$ci[3,],col=3,lty=2)	## upper CI
  lines(data$xpred,data$pi[1,],col=4,lty=2)	## lower PI
  lines(data$xpred,data$pi[2,],col=4,lty=2)	## upper PI  
  abline(b0,b1)				## true model  
  legend("topleft",legend=c("True Process","95% Credible Interval","95% Prediction Interval"),col=c(1,3,4),lty=c(1,2,2))
}
plot_stuff(output)
```

### Lab Report Task 2: Power Analysis
The pseudodata we generated had a fairly large sample size (n=500) which allowed us to get a precise estimate of the true model.  An important question in experimental design is what sample size is required in order to estimate model parameters accurately and to the required degree of precision.  To take a rough first pass at this I'd like you to re-run this analysis two more times with a sample size of n=50 the first time and n=5 the second time.  (note: I'll be asking you to make comparisons between the models so you'll want to take a look at what information you'll need to save from each run before re-running the model)  For each run check the MCMC diagnostics and then report the following



### Task two results: 
Step one is to functionalize our previous steps.

Step two is to rerun and shove all the results into a list.

Answers strewn throughout

```{r}
L <- list()
for(n in c(5,50, 500)){
  # Setup the different data sets
  d<-gen_data(n, beta, sigma2)
  d <- append_priors(data = d, bprior = bprior, vinvert = vinvert, s1 = s1, s2 = s2)
  d <- precompute(d)
  d <- gibbs(data = d, sinv)
  d <- credi_predi(d)
  L[[length(L)+1]] <- d
}


# A plot with the pseudo-data, the true model, the 95% credible interval on the model, and the 95% prediction interval.

par(mfrow=c(1,3))
for(l in L){
 # plot them
 plot_stuff(l)
}

# Table it up
table_stuff<-function(L){
  table <- data.frame()
  for (l in L){
    n <- length(l$x1)
    sumB<-summary(mcmc(l$bgibbs))
    sumS<- summary(mcmc(l$sgibbs))
    cols_1 <- rbind(sumB[[1]][,1:2], sumS[[1]][1:2])
    cols_2 <- rbind(sumB[[2]], sumS[[2]])
    df <- data.frame(cbind(cols_1,cols_2))
    names(df)[3:7] <- dimnames(cols_2)[[2]]
    row.names(df) <- NULL
    df <- cbind(Var= c("b0","b1","sigma2"), df)
    df$n = rep(n,3)
    table <-rbind(table, df)
  }
  table$CI <- table[,"97.5%"]-table[,"2.5%"]  
  return(table)
}
table <- table_stuff(L)
# A summary table of parameter estimates
table[order(table[,"Var"]),]

library(ggplot2)


p1<- ggplot(data=subset(table, Var=="b0"), aes(x=log(n), y=Mean)) + geom_point() + geom_line() + geom_errorbar(aes(ymax=`97.5%`, ymin=`2.5%`, width=0)) + theme_bw() + ylab("b0") + geom_hline(aes(yintercept=10, color="red")) + geom_line(aes(y=`97.5%`), colour="green")+ geom_line(aes(y=`2.5%`), colour="green")

p2<- ggplot(data=subset(table, Var=="b1"), aes(x=log(n), y=Mean)) + geom_point() + geom_line() + geom_errorbar(aes(ymax=`97.5%`, ymin=`2.5%`, width=0)) + theme_bw()+ ylab("b1")+ geom_hline(aes(yintercept=2, color="red")) + geom_line(aes(y=`97.5%`), colour="green")+ geom_line(aes(y=`2.5%`), colour="green")

p3 <- ggplot(data=subset(table, Var=="sigma2"), aes(x=log(n), y=Mean)) + geom_point() + geom_line() + geom_errorbar(aes(ymax=`97.5%`, ymin=`2.5%`, width=0)) + theme_bw()+ ylab("sigma2")+ geom_hline(aes(yintercept=16, color="red")) + geom_line(aes(y=`97.5%`), colour="green")+ geom_line(aes(y=`2.5%`), colour="green")

library(gridExtra)

# Plots showing how the estimate of the parameter mean and the 95% CI around that mean changes with sample size.  Sample size should be on the x-axis on a log scale.  Make plots for each of the 3 parameters
grid.arrange(p1,p2,p3,ncol=3)

```

The 95% CI and PI intervals always appear to contain the model. The log-sample size appears to narrow the CI exponentially.

**What is the (approximate) minimum sample size that would be required to reject the hypothesis that the slope of this line is 3/2 with 95% confidence.**

See the lower green line (2.5% CI) on the b1 plot (center). This line has a value of 1.5 around log(n) = 3.5 or n = `r exp(3.5)`. Therefor a sample of about `r exp(3.5)` is required to reject a slope of 1.5.



### Extra Credit: Lab Report Task 3:  Informative Priors
Suppose a review of the scientific literature reveals that 95% of the reported values for the slope of the relationship between these two variables fall between 1.5 and 2.5.  For the case where n=5 re-run your analysis with the same pseudodata as you did in Task 2 above but incorporating this additional information as a prior constraint on the slope.  Include the following in your report

```{r}
# new prior
new_bprior <- c(0,1) # should be (0,2)
new_vinvert <- solve(diag(c(1000,0.25))) # Should be 0.25^2 (since 0.25 is SD implied by 1.5-2.5)

# Create the data object
  d <-gen_data(5, beta, sigma2)
  d <- append_priors(data = d, bprior = new_bprior, vinvert = new_vinvert, s1 = s1, s2 = s2)
  d <- precompute(d)
  d <- gibbs(data = d, sinv)
  d <- credi_predi(d)
  
  # Summary Table
  new_table <- table_stuff(list(d))
  new_table

  # Compare to the sample without prior
  table[table$n==5,]

  # Plot
  par(mfrow=c(1,1))
  plot_stuff(d)
```

**Describe how the inclusion of prior information affects the estimate and CI for all of the parameters (not just the slope) and the overall CI and PI on the model**

Using the two tables and the plot it becomes obvious that the estimates of both slope and intercept are heavily weighted towards the pior. The credible interval and the prediction interval have also narrowed considerably. The effect of the slope (b1) prior on the intercept (b0) posterior is probably due to the correlation between b0 and b1.

It should be noted that on my first run I coincidentally got the posterior converging directly to the true process model. On subsequent runs it became clear that this was just luck of the draw.
